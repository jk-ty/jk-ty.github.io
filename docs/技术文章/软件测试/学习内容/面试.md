# 面试

## 1、面试

```
面试官您好，我叫马广涛。·目前，我已经从事软件测试工作快两年的时间了，在工作中我主要负责web网站、app、小程序
测试以及接口测试、弱网测试，我也曾参与过性能并发方面的测试，在工作过程中我也掌握了一些技能和工具来辅助我的工
作，熟练掌握测试的流程，使用测试用例的设计方法，进行分析设计测试用例，并撰写测试用例，能够独立完成测试工作任
务，使用Linux来进行日志查看，使用navcat进行sql语句的编写，使用jmeter做接口测试、数据的抓包，定位前后端
bug我一般使用fidler，用monkey做app的稳定性方面的测试，qnet对app做弱网测试，solopi做app性能监控，小程
序方面我们微信开发者工具来进行测试工作。性格方面：我平时比较开朗，熟悉了之后可能会有点话多，平时也喜欢打羽毛
球，和朋友一起外出走走，看看书了解一些自己感兴趣的知识。这就是我的一个简单的自我介绍。
```

## 2、公司人员组成

```
我们公司就是只有一个研发部，因为公司比较小

我们研发部人员有项目经理1名、1名产品经理、数据库1个、前端6个（2个IOS,2个Android,2个web（两前台一个后
台））后端4个，测试人员（3个）运维工程师（1个）

项目经理（1人）；产品经理（1人，主要就是将需求转换成产品原型图，使用墨刀）；UI设计师（2名）主要就是设计界
面，p图；前端开发（2名）；后端开发、接口（3名）；软件测试人员（2名）；运维工程师（1名）。
```

## 3、 需求从哪里来的

```
项目初期时由产品经理和客户沟通整理编写出的一份需求文档，根据需求文档，产品经理会出一份产品原型图，最终项目组
人员会在需求评审之前拿到需求文档，然后在需求评审会议上进行需求评审
```

## 4、需求评审 测试做什么

```
测试主要关注需求的功能是否易于理解，功能间是否存在冲突，业务间逻辑是否通顺，需求描述是否清晰准确，是否有二义
性描述。功能是否有明确的约束限制。比如输入内容长短，组成等。项目周期，任务优先级，提测时间是否明确，各开发及
测试人员的任务划分情况。
二义性（歧义），提测时间是否明确
```

## 5、 如果需求不明确怎么处理?

```
需求不明确的话我会在需求评审会议上面提出来，问清楚这个需求，只有明确需求，
才能更好的完成工作，后续工作中还是不清楚，可以找产品再去确认这个需求。
```

## 6、 测试工作中没有需求时该怎么办呢?

```
分为:已开发阶段和未发开阶段

已开发阶段:首先需要根据实际的项目,去了解项目的流程以及业务,使用Xmind工具去划分相应的模块,提取出功能点和测
试点,如果有不明确的和团队人员(包括项目组内的测试人员和开发以及产品充分交流),了解项目的业务及各项功能.再整理
出测试文档

未开发阶段:了解项目的基本情况,项目的背景,充分和团队成员(包括项目组内的测试人员和开发以及产品)讨论了解熟悉业
务,同样使用Xmind工具去划分相应的模块,提取出功能点和测试点和整理出测试文档,在过程中如果遇到问题也要充分的和
团队成员以及产品经理沟通.
```

## 7.如何写好测试用例

```
参考需求文档,详细了解需求和业务,首先用xmind划分模块和测试点，再运用好测试用例设计方法(等价类,边界值,判定
表,场景法,错误推测法等),一定要步骤、场景清晰、尽量去覆盖所有的测试场景,另外在设计时既要考虑功能又要考虑非
功能质量特性(界面,易用性,兼容,网络,安全,性能)方面

覆盖到所有的业务逻辑（包括正常逻辑和异常逻辑）。

覆盖到所有的典型用户场景。

覆盖到所有的需求点

没有冗余的用例。
```

## 8.之前测试用例用什么写的？

```
有两种方法，一种是用Excel编写用例，另外一种用TAPD编写测试用例进行管理的
```

## 9.测试用例是怎么评审的？

```
5W1H

一般会在编写完测试用例后会展开测试用例评审工作，我们会和相关开发人员和测试小组内的人员参见测试用例评审会
议，大家来阐述自己的测试用例，过程中找出重复的测试用例并且补充未覆盖的测试点以完善测试用例的覆盖率，来提升
测试用例的质量。

用例评审参加人员：主要是产品、开发（前端和后端）、测试、项目负责人、运营。

用例评审时间：在半小时到一个小时左右。或者对功能点划分优先级，优先评审

优先级高的用例，再针对疑问多的用例评审，最后对于功能简单的用例可简单带过。

用例评审形式：对照测试用例，从上而下，从左到右，逐条念；先对功能复杂，优先级高，疑问多的用例进行评审，再评
审功能简单，优先级低的功能点；

评审后的事情：评审结束后，第一时间整理测试用例，把修正的内容重新整理补全。修改的功能点用黄色标记。会上未确
定的内容，会后继续跟进，直到确定结果。如果有遗漏的功能点，新增后用绿色标记。没有任何有疑问的地方了，再做个
简单的用例评审会议总结，如修正了哪些功能点、补全了哪些、哪些模块功能有变动、哪些功能推迟到下一期做等
```

## 10.每天写多少条测试用例？

```
没留意过具体每天具体些多少条。一般我2、3天写一个模块，一个模块的测试用例大约在150到200条左右。每天大概
50~100条左右,因为每天除了写用例以外还要做其他的工作.
```

## 11.整个项目写了多少用例，你负责的模块大概写了多少用例?

```
[切记根据自己的项目及负责的模块来]

答：这个得根据项目的复杂程度，我们最近做的这个也还好，整个项目写了大概1干5百多条，我负责的几个模块加起来总
共写了6百多条(你要思考，你负责了哪些模块，大概评估下，不要乱喊)。

总结注意点：没有标准答案，先说你的前置条件，再说数据，只要你前置条件和数据匹配即可。
```

## 12.如何保证测试质量(或100%覆盖了需求)

```
	把需求了解通透，引用测试用例评审机制，然后编写测试用例的时候用测试设计方法,比如边界值，用等价类补充一些
用例，根据过往经验用错误推断法来追加一些用例，如果存在组合情况的话我会用因果图或者判定表来编写，如果业务场景
清晰的情况下我会用流程分析法，如果状态有发生改变的话我就会用场景法覆盖。要考虑到各种场景，全面覆盖到会出现的
场景。如果人员充足还需进行交叉测试
```

## 13.如何提交一个好的BUG？

```
对BUG有一个清晰明了的描述；详细描述BUG重现的步骤

对于产生BUG的环境进行描述；提交BUG相关的录屏和错误截图;

定位好BUG的等级；将预期结果与实际结果进行对比。
```

## 14.你们用的什么bug管理工具，都用来干什么？

```
我上一家公司用的是禅道项目管理工具,我们测试主要用来编写和管理测试用例以及进行bug管理(禅道的账号是在我刚入职
的时候运维提供的，登录之后把我拉到项目里面，给到我们测试对应的权限)　在产品导入需求之后，我们就可以开始在需
求上编写针对于该需求的测试用例，在开发提交测试单后，执行所有测试单中的测试用例，提bug，提交bug给开发人员，
根据需求所关联的开发人员，可以找到该bug指派的对（bug创建后管理整个bug的生命周期）
```

## 15.缺陷严重等级

```
致命：系统崩溃，数据丢失，数据毁坏，无法运行等Bug。

严重：功能和性能不能实现。次要功能全部丧失。功能遗漏等等。             

一般：一般缺陷是指不影响产品的运转和运行，不会成为故障起因，但对产品的外观功能影响较大的缺陷。实际测试中存在
最多，虽然不影响系统的基本使用，但是也达不到预期效果。

轻微：如文字排列不整齐，有个别错别字等。
```

```
电商项目Bug严重级别的制定可能如下：

高严重级（Critical）：指缺陷会导致系统崩溃或严重损坏，或者导致数据丢失或不一致。

系统出现严重故障，无法启动或者崩溃，导致用户无法正常使用系统或进行购物。


中严重级（Major）：指缺陷会导致系统功能无法正常使用或者功能出现明显异常。

商品价格信息显示不准确或者缺失，导致用户无法进行购买和下单。

用户订单状态显示不准确或者缺失，导致用户无法准确了解订单状态和物流信息。

低严重级（Minor）：指缺陷影响较小，不影响用户正常使用系统和购物流程，但仍需要进行修复。

页面样式出现缺陷，如图标大小不匹配、颜色搭配不合理等，影响用户体验。

商品描述信息中出现拼写错误、语言表达不够优美等问题，影响用户购物体验。

用户在一些特殊场景下操作时，可能会出现一些小问题，但不影响整个购物流程。

未指定严重级（Unassigned）：指缺陷还没有被指定严重级。

测试人员发现了一些系统缺陷，但还没有进行评估和分类。
```

## 16.缺陷优先级

```
P0—紧急：完全不能满足产品要求，基本功能明显未实现或完全不可用，阻塞测试流程与进度（核心功能流程）

功能未实现 、功能缺失,业务流程不正确,闪退,越权,信息丢失,安全问题（XSS、SQL等注入）,服务器500错误等

P1—高：产品的功能实现和需求不符合，没有达到预期的效果，但不阻塞测试进度（非核心功能流程，不影响其他功能）

小功能未实现，小功能缺失,概率性的闪退,页面显示（未按照UI实现，用户体验较差）,页面跳转错误（示例：
FaceBook打开计算器跳转到了官网）,排序错误, 明显的卡顿（可稍微忍受）,系统兼容性问题导致次要功能异常（如：
页面未显示全，但是不影响功能的使用）,异常情况处理缺失，如断网、弱网、中断操作（前后台切换）,类型、边界条件
下发生错误

P2—中：比较小的功能、UI或交互问题，可以绕过此类问题来进行测试。

概率极低的偶现bug或闪退,占比率低的非主流系统兼容性闪退（Android的某些机型）,提示文案错误,字体大小或排版
不统一,文案过长被遮挡、未截断或未换行,删除未给出二次确认,非常规操作或非常规路径、如多步复合操作后才能复现
的问题（用户一般不这么操作）,UI兼容性/适配问题

P3—低：一些可修改或不可修改，或者是还不确定能否修改成功的bug，均不影响用户体验使用
极少机型的适配问题,建议类bug，可修可不修，修了最好，不修不影响发布版本,延期处理的缺陷（开发确定是缺陷，不
影响当前上线，开发可在上线后处理）,挂起的缺陷（开发人员待找技术解决方案）
```

```
电商项目Bug优先级的制定可能如下：

高优先级（High）：指缺陷需要尽快解决，因为它对用户的体验或系统的核心功能产生了显著的影响。

商品下单付款流程出现缺陷，导致用户无法完成支付，无法下单购买商品。

用户注册、登录功能出现缺陷，导致用户无法成功登录系统，无法使用购物车和收藏等功能。

商品信息显示出现缺陷，导致用户无法正常浏览商品信息，无法进行购买和下单。

中优先级（Medium）：指缺陷需要在较短时间内解决，因为它对用户的体验或系统的常规功能产生了影响。

系统性能出现缺陷，导致页面加载时间过长或出现卡顿现象，影响用户体验。

商品搜索功能出现缺陷，导致用户无法精准搜索到商品，或搜索结果出现错误信息。

订单状态更新出现缺陷，导致用户无法及时获取订单状态信息，不能及时进行订单跟踪。

低优先级（Low）：指缺陷对用户的体验或系统的功能产生的影响很小，可以在一定时间内不紧急解决。

页面样式出现缺陷，导致页面排版错乱或字体颜色不够美观。

一些功能提示信息出现拼写错误或语言表达不够优美。

一些页面链接出现缺陷，导致页面跳转出现错误信息。

未指定优先级（Unassigned）：指缺陷还没有被指定优先级。

测试人员发现了一些系统缺陷，但还没有进行评估和分类。
```

## 17.测试开启结束挂起标准

```
开始标准:开发完成编码可提测

系统测试挂起标准:基本功能测试不能通过；出现致命问题导致30%以上用例被堵塞，测试无法执行下去。

系统测试恢复条件:基本功能测试通过，可执行进一步的测试。

结束标准:测试完成并且测试用例要完全覆盖需求，测试用例要100%执行, BUG修复率在95%以上，并且不存在一般级别以
及以上BUG, 一般级以下BUG要经过产品经理及项目负责协调决定可遗留数量
```

## 18.发现哪些映像比较深刻bug/经典bug?

```
1.数据漏洞:APP,用A账户登录,操作平台相关数据,手动退出之后,用B账户登录,访问APP,部分页面中会遗留A用户相关
的数据和信息.退出再登录新用户,开发没有清理本地缓存的客户数据.

2.某些表单功能：一开始没有限制字符的类型和长度，当输入超长的字符，提交后页面直接抛500,ERROR错误。前端没有
根据数据库数据长度和类型对数据进行合理限制.

3.翻页功能: 翻页时始终少一个数据,后台在做分页接口时limit的序号没有从0开始计算

4.非必填字段: 有时非必填字段如果不填写内容提交,返回查看列表数据,未填如的数据显示为null(其实应该显示为
空),后端没有对null数据进行处理前端直接显示了

5.弱网下，点击提交数据，有时会提交两次。因为前端在弱网环境下提交后后端没有反馈进行了重复提交，但其实后端已
经接收到了前端第一次提交的数据，只是因为网络问题没有反馈结果。开发的解决方法是进行同一个业务连续提交时验证
请求的时间间隔。如果小于2秒则不再处理。
```

## 19.上线后发现bug，如何处理？

```
首先观察bug出现的原因，看是否可以从现有版本流程上规避这个问题，让客户可以暂时绕过问题正常使用，不耽误用户
的操作。

看严重级别：严重还是不严重

严重的：紧急变更上线，比如影响用户正常主要功能及流程的。

不严重：修复好后跟下个版本一起上线

一般用户会通过客服反馈到项目组这边，项目经理会根据功能模块的负责人，分给对应的开发与测试。

测试人员：编写对应的测试用例、测试环境中重现bug、提交bug、

交给开发进行修复、修复完成bug、进行bug的复测。

如果测试环境无法重现，可以把生产环境出问题模块的相关的数据放到测试环境中测试，如果是APP的话,有可能是机型的
兼容问题,我们可以通过云测平台,找到用户所使用的机型,进行远程真机测试.

还是不能复现，直接测试生产环境来定位问题。
```

## 20.难以重现bug（偶现bug），怎么处理?

```
首先，我会描述好bug信息进行提交，对bug多测几次，如果是偶现的话要详细描述下偶现步骤，偶现率，然后提交给开
发。如果测了很多次都无法重现的话我就先把bug挂起，并且留意一下，看看往后的测试中，如果在后面的测试中重现bug
就激活，如果经过几个版本都还没发现的话就关闭bug。
```

## 21.一个项目总共有多少bug?

```
一般项目前期bug会多点，一天，三四十，二三十都有（包括功能和界面），后期相对较少一天一二十或者十几个。总共具
体记不太清楚

质量好的时候会少一些大概五六十条,质量不好的时候100到130左右（严重10%，一般70%，轻微的20%）个
```

## 22.Cookie,Session,Token区别?

```
cookie是保存在客户端，客户端是可见的，session是保存在服务器端，客户端不可见

cookie有大小限制，最大4K ,session没有大小限制

session更加安全，但过多时会比较占用服务器性能

cookie过期机制为设置的过期时间。session的过期时间机制可以叠加

token是用于APP
```

测试点

```
1、登录成功后，服务端是否生成有效的session?session信息是如何存储的？

2、服务端是否对session设置了过期时间？一种是不管是否进行会话，到了设置的固定时间都会过期；一种是多长时间
不进行会话，session就会过期。

3、退出登录，服务端应该清除或者无效session

5、客户端cookie是否设置了过期时间？（如何不设置过期时间，关闭浏览器就会清除cookie;如果设置了过期时间，到
固定时间才会过期）

6、cookie没有设置过期时间，退出登录时，客户端是否清除cookie

7、cookie中的是否有敏感信息？或者敏感信息是否加密？

8、cookie的HttpOnly是否设置为true（防止cookie值被页面脚本读取）

9、cookie的Secure是否设置为true（保证cookie与WEB服务器之间的数据传输过程加密）

10、考虑客户端禁用cookie的情况？

11、session和cookie需要考虑多端登陆？
```

## 23.接口测试流程

```
首先从开发那里拿到接口文档(word,swagger),然后我们首先熟悉接口内容(接口功能,接口地址,请求方式,请求参数,返
回的数据格式,状态码,返回的示例模板等),之后编写测试用例,再根据测试用例执行接口测试,常用工具有
(jmeter,swagger,postman,apifox,apipost).进行bug管理,输出测试报告
```

## 24.接口测试要注意点,测试点？

```
1接口测试的时候，关注参数的空值, 默认值、必填项，类型，长度，组成规则，业务约束(必须填写存在的ID), 接口业
务(登录5次后禁止登录) 安全(是否加密), 幂等性测试(重复提交),越权访问(使用其他权限token越权访问越权的一些接
口,比如用普通用户token访问管理员功能接口), token失效验证, 错误状态码的覆盖.  新旧版本接口兼容.
2接口返回状态码，返回数据的内容及排序
3接口返回的数据是否和数据库中一致
```

## 25.接口测试用例怎么设计？

```
首先了解接口文档，主要需要关注的测试点有：
1）关注参数的空值, 默认值、必填项(必填项不填)，类型，长度，组成规则，业务约束, 安全测试（敏感数据是否加
密），幂等性测试(重复提交),越权访问(使用其他权限token越权访问越权的一些接口,比如用普通用户token访问管理
员功能接口), token失效验证, 错误状态码的覆盖.  新旧版本接口兼容，中英文及特殊字符.
2）逻辑业务，结合需求文档进行确认（例如：登录5次错误应该出现15分钟后才能登录的提示）（购物下单付款后需要确
认下单是否成功和积分是否有增加）
3）接口返回状态码，返回数据的内容及排序
4）接口返回的数据是否和数据库中一致
5）入参和出参的准确性
就是通过以上这些测试点来设计测试用例的
```

## 26、接口出现异常，如何分析？

```
一种方案使用工具(fiddler)抓包，查看请求报文和返回报文

第二种：web端直接使用浏览器 F12界面的Network，查看请求报文和返回报文

```

## 27.接口测试中发现的问题,bug?

```
一般错误：接口没实现，没按约定返回结果，边界值处理问题等
输入异常值（空值、特殊字符、超过约定长度等）
输入错误的参数，多输入，少输入参数
安全性问题，如明文传输、返回结果含有敏感信息，没对用户身份信息做校验，没有恶意请求拦截(时间范围内请求次数限制)等
选答 性能问题，如接口并发插入多条相同操作，响应时间过长，接口压测出现瓶颈等
例如：一个管理员删除操作，前端界面非管理员不显示删除按钮，通过接口可以使用非管理员进行删除
```

## 28.接口测试的重点和难点是什么？ 

```
重点 要分析好接口文档内容,工具的灵活使用
难点 对前、后duan的数据交互过程、后端业务处理逻辑非常清楚。对关联接口，要清楚接口之间的数据依赖关系.
```

## 30.JMETER使用?

```
（1）在测试计划下新建一个线程组。
（2）然后就是新建一个HTTP请求默认值。（输入接口服务器IP和端口）
（3）添加HTTP信息头管理器添加登陆后的token值，一般请求参数是json时还需要设置content-type为
application/json 
 (4)  再新建更多HTTP请求，一个接口创建一个（输入接口路径，访问方式，参数等），可以将设计好的测试用例数据
 通过CSV参数化的方式添加到请求中。
（5）如果接口间有相互数据调用关系，需要用到关联，通过json提取器或者正则表达式提取器提取之前请求返回的数据
保存至局部变量，需要调用的接口通过 ${变量} 的方式调用参数。如果是线程组之间的调用，需要先使用setproperty
在beanshell中将局部变量提升至全局变量，然后通过property调用全局变量
（6）然后创建断言和查看结果树。
```

## 26.接口测试报告？

```
我一般使用ant自动生成接口测试报告，需要创建ant所需的build.xml填写jmeter路径，脚本路径，测试结果输出路
径，测试报告输出路径等信息。修改jmeter配置文件jmeter.properties，将注释的返回结果相关数据配置放开。执
行ant命令生成接口测试报告。
```

## 33.怎么实现接口自动化

```
1）先从开发拿到接口文档，了解接口文档，比如接口路径、资源路径、状态码、示例模板、参数等，再编写好测试用
例，

2）然后在jmeter中编写脚本，给每一个接口添加一个线程组，在每个线程组添加一个请求，通过CSV文件进行参数化，
将接口测试用例中的测试数据添加到CSV文件中，并调整线程组中的循环次数与测试用例相一致，

3)再添加断言，如http状态码断言和响应信息断言，判断响应的数据以及状态码是否正确，

4)对于固定不变的数据还可以添加请求默认值，比如协议，服务器IP地址和端口号

5)如果接口间有依赖关系，通过关联方式对数据之间进行关联，比如用户token值，使用json提取器或正则表达式提取
器提取相关数据，并通过setproperty赋予变量，再通过beanshell赋予全局变量，在需要使用的地方通过
property获取数据，

6)还可以添加请求头管理器，比如添加登陆后的token值，一般请求参数是json时还需要设置content-type为
application/json 格式

7)然后添加察看结果树查看结果，最后使用ant自动生成测试报告
```

## 35.你之前是怎么做性能测试？

```
我们之前项目的业务量并不大,公司也没有性能测试的要求。

参与过性能测试，之前性能测试都是由我们测试老大负责的，我有协助过我们老大一同进行过性能测试。我也有自己通过
业余时间学习了解过相关性能测试知识.

	之前的性能测试流程是这样的

	1.分析业务场景，制定一个性能测试的计划或方案（要测试哪些模块，前期需要构造测试数据.保证平台有一定的
用户量与数据量）

	2.创建测试脚本（对脚本进行优化：参数化，关联，断言，事物控制器，集合点，定时器）

	3.定义性能测试场景，进行并发测试

 找出系统的最大/最佳并发用户数的做法： 一般会创建线程组添加测试请求，添加事务控制器，在事务控制器中添加多
 个业务相关的请求，添加固定定时器（思考时间）用于模拟用户正式操作行为，添加同步定时器（并发）。设置线程组
 线程数（用户数），修改线程持续运行时间

   多次复制以上线程组，递增调节线程数量以及同步定时器并发数。

	4.生成测试报告，分析测试结果（开发，测试一起沟通分析）

最大并发用户数大致在200~300 TPS在2400左右
```

## 36.什么是吞吐量(TPS)？怎么判断服务器性能？

```
	吞吐量(TPS)：单位时间内，服务器处理所有客户请求的数量，吞吐量越高服务器性能越好

一般我们统计事务的响应时间，响应时间越快服务器处理能力越强

	首先，一般我们会结合多个指标：响应时间(2-5-8原则),请求错误率（2%,5%），CPU（70%、85%、90%），内
存使用稳定（70%），TPS，等综合指标来判定；

找出系统的最大/最佳并发用户数的做法：

    随着用户数增加，以上指标都在可控的范围，并且，我们的TPS/吞吐量随着用户的增加而线性增加|，此时如果响
应时间也在3S以内，CPU使用在70%以内，内存使用稳定，此时对应的并发用户数就是我们的最佳并发用户数；

    随着用户数增加，以上指标都在可控的范围，并且，我们的TPS/吞吐量随着用户的增加而增加，并且增加的相对
缓慢，此时如果响应时间在5S以内，CPU使用在90%以内，内存使用稳定，此时对应的并发用户数就是我们的最大并发用
户数；

    以此方式来判定服务器性能；
```

## 39.如何进行性能调优的?

```
性能调优的工作一般是由我们测试老大(或性能测试工程师)来完成的,我也了解过具体调优的一些方式,一般用排除法

	1、首先考虑网络方面问题：使用ping命令查看与目标服务器的连接是否正常、传输速度的快慢。通过提升服务器
的带宽，看响应时间是否相应降低。

	2、考虑数据库的问题，可以单独去压测数据库，查看数据库的最大连接数和SQL语句的执行时间，索引命中率和
sleep等待时间等

	3、考虑 Apache中间件的问题，查看中间件设置的最大连接数是否合理，如果设置的连接数太小，会话数超过设
定的最大连接数时会导致等待时间变长，出现响应超时情况

	4、考虑服务器的硬件配置，如内存、CPU、磁盘读写速度等，可以用top命令来监控，也可以使用jmeter的
perfmom工具来监控，它会把监控的数据图表形式，方便我们查看。

	5、最后考虑开发代码写的好不好，处理时间长不长的问题。

	负载测试：对系统不断增加压力或增加压力下的持续时间，直到系统的某项或多项性能指标达到安全临界值，例如
某种资源已经达到饱和状态

	压力测试：评估系统处于或超过预期负载时系统的运行状况，关注点在于系峰值负载或超出最大载荷情况下的处理
能力
```

## 40.简述fiddler主要用来做什么？

```
 1）抓包进行数据测试，定位前后端bug, 验证请求数据的正确性, 敏感数据的安全性.
 2）mock测试，构造服务器响应的内容; 比如第三方接口没有完成,比如第三方接口需要付费调用的时候.
3) 断点调试；篡改请求及响应数据，查看应用程序是如何处理的。修改请求及响应信息,模拟不同的数据,不同的错误信
息.
4) 发送请求，做接口测试；
5) 限速做弱网测试；
6) 前端性能数据, 统计发送包接收包大小，请求-响应时间，统计服务器处理时间；
```

## 41.fiddler怎么做弱网测试？

```
 我们一般是进行APP的弱网测试
 	1.首先在Fiddler中开启抓取和解析https选项。然后开启允许远程设备连接选项  
	2.重启fiddler，将手机和电脑连接在同一局域网内 
 	3.查看本机ip地址，打开手机的WiFi设置，配置代理为电脑ip，端口为Fiddler默认端口 
	4. 通过手机浏览器访问IP地址加8888端口号下载安装fiddler证书 
 	5.修改Fiddler脚本，设置上传和下载的参数数据，然后开始限速选项，然后打开手机app，就可以进行弱网测试了
```

## 49.判断是前端还是后端BUG？

```
浏览器自带的F12或者fiddler抓包。主要从请求接口、参数、响应三个方面分析。
抓到的请求接口的URL，请求方法，请求头，参数，是否正确，如果错误，那就是前端问题。
如果请求数据都正确，但是响应数据错了，那就是后端问题。
如果请求数据正确，响应数据也正确，但是前端界面对应展示的数据错误那就是前端问题。
还可以通过状态码判断，5开头的（5xx）一般是后端bug,4开头的（4xx）一般是前端bug.
```



## 58.兼容性测试怎么做的？

```
我们之前做兼容测试从web和app进行区分
web兼容性考虑不同浏览器不同浏览器版本以及不同的浏览器内核和分辨率，
浏览器内核分为：IE内核，谷歌（Chrome）内核和火狐（FireFox）内核，
关注市面上主流的浏览器：360、IE、谷歌、火狐、QQ、百度等，
还要考虑分辨率，（1920*1080,1366*768，1536*864，1280*720，1600*900）等
APP也需要考虑系统和系统不同的版本和不同的手机品牌以及屏幕尺寸和分辨率
系统和系统不同的版本分为安卓（10、11、12、13）、iOS（12、13、14、15、16），
安卓不同的手机品牌（华为、OPPO、vivo、小米、魅族等）
屏幕尺寸：6.4寸，6.6寸，6.7寸，6.8寸 （包含刘海屏/无刘海屏/折叠屏）
分辨率：1080*2340，1080*2400，1080*2480,1440*2560，1440*3200等
当然我首先要确认需求，如果需求定义了就按需求执行
如果需求未定义,可以百度搜索当前浏览器使用排行,前5到前10.或通过大数据平台 （百度流量研究院或者安兔兔）
```

## 87.给你一个项目，你怎么展开工作(怎么测)

```
面试官考的点：进我公司，给你一个项目，你怎么做，看你有没有独立负责项目能力
第一点：首先我要熟悉这个项目的背景，我要了解这个项目是做什么的，项目的类型是什么，充分了解项目需求，参与需
求评审或和产品经理或项目组讨论，前期时间多投入点，后面风险就小一点。
第二点：熟悉这个项目的核心模块和业务，并划分模块的一个优先级，先测试哪些核心模块，再测哪些模块
第三点：根据交付的时间节点，我会评估一下工作量，需要做一个测试计划。包括整体测试范围，时间安排，以及风险预
估等，计划完成后我会给到项目组或项目负责人进行一个计划评审，看安排是否合理
第四点：接下来，按照计划执行，编写用例，用例在编写过程中要充分结合需求，采用测试用例设计方法（等价类，边界
值，场景法等），同时也要考虑数据交互，业务约束等，功能覆盖好后还要关注非功能质量特性（界面，易用性，兼容
性，网络，安全，性能等）。测试用例编写完成后组织项目组召开组内测试用例评审，开发最好可以参与，可以对用例进
行查漏补缺。接着就等开发提测后进行测试执行，跟踪管理好bug状态，及时和开发沟通解决问题，督促开发解决问题提
升项目完成效率。后期做好回归测试和交叉测试。测试完成后输出测试报告。给出最终产品质量情况和是否可上线结论。
如果可以上线，那么还需对线上环境进行一到两轮的系统回归。我基本是这么做的
```

## 96.对公司或项目组贡献

```
推进了项目进度

一开始公司关于提测制度并不完善，经常是边做边改边测，导致测过的模块反复出现bug，不但工作繁琐并且进度缓慢。

后期我建议在测试前开发需花费一点时间

制定提测标准，对修复的问题进行描述，内容包括修改及涉及改动的所有模块。这样我们就把重点放在修改及关联的最有可
能出现问题的模块上，一定程度上提升了效率及质量。
```

## 97.职业发展规划

```
长期规划：先做好自己的本分，多参与公司项目，尽可能帮着公司解决问题，希望在公司中通过自己的努力，往管理层发
展，争取2-3年成为测试组长，5年内争取成为测试经理，具体的发展方向，还需要公司的发展方向去调整。
```

```
如果公司只做功能测试
·
接下来一年时间内，更加完善自己的功能测试，2-3年内能够掌握自动化技术

找一个比较稳定平台，跟公司长期发展，后期如果有机会也期望可以走管理或者产品路线
```



## 98.这几年测试收获或心得

```
我觉得做好一个测试首先要有一个良好的态度，同时还要细心，同时还要有足够的耐心以及良好的沟通能力。最后还要有较
好的学习能力，产品质量，测试流程这块很关键，规划好测试计划也是关键。
```

```
我觉得态度比能力要重要，做好一个测试，最主要是性格，信心，耐心，细心还需要良好的沟通能力。不断学习的能力，产
品质量，测试流程这块很关键，规划好测试计划也是关键。
```

## 99.你还有什么问题要问?

```
1.公司现在做什么项目，主营业务
2、公司的业务方向是什么？
3、如果我入职之后，我的工作职责是什么？
7.面试没有回答上的问题，再去请教
```



```
1.公司现在做什么项目，主营业务		

2.公司目前做哪方面测试

3.公司这边测试人员分配比例

4.进入公司，我这边大概的工作安排

5.公司后续发展方向

6.有没有培训

7.面试没有回答上的问题，再去请教

简历项目经验

测试用例，接口，兼容，数据库，弱网，安全，埋点。
```



## 100.简历中工作业绩描述（STAR法）



```
Situation（情境）：在公司担任软件测试工程师工作，负责测试公司产品的各个方面，把控项目及产品质量。

Task（任务）：作为测试工程师，我的任务是尽快测试产品的各个方面，并确保产品符合公司的高质量标准。我们公司
的时间表通常都非常紧迫，需要在短时间内完成测试和发布产品。

Action（行动）：为了保证测试任务进度及质量，我采用测试用例设计方法设计测试用例，结合各种测试相关工具辅助
测试工作,比如Jmeter,Navicat,ADB,Monkey,Fiddler等工作完成功能，界面，易用性，安全，网络等多维度测
试，结合自动化测试方法来测试产品的各个功能。我使用了自动化测试工具并编写了多个自动化测试脚本来测试产品的各
个方面。我还使用了持续集成和持续交付工具，以确保每次更改都经过了自动化测试和部署。进行测试报告的总结归档

Result（结果）：通过不懈努力，我成功地在任务时间内完成了整个产品的测试，并确保产品质量符合公司的高标准。
公司按时发布了产品，取得了客户的好评和市场的认可。我的行动保障了公司各个产品的质量，同时获得了公司及同事的
认可和信任
```

## 